{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWuqOF-dyEPr"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe602l5jyCc_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TADgQXv-xMjV"
      },
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V48UrgdrtRXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df = pd.read_csv('/content/drive/MyDrive/ulasan_aplikasi.csv')"
      ],
      "metadata": {
        "id": "kPDmyT9gtTy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtF3LY8WxKsh"
      },
      "outputs": [],
      "source": [
        "main_df.info()\n",
        "print('Jumlah nilai terdupliasi: ', main_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5IMBh3UOf1l"
      },
      "outputs": [],
      "source": [
        "main_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24v-jlfz3t11"
      },
      "outputs": [],
      "source": [
        "print(main_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBv9RgkfxuMX"
      },
      "source": [
        "## Preprocessing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JadXh4keFVLf"
      },
      "source": [
        "### **Case Folding**\n",
        "Proses mengubah semua huruf dalam teks menjadi huruf kecil atau huruf besar agar konsisten. Misalnya, mengubah \"TeKS\" menjadi \"teks\" atau \"TEKS\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAUPZ0sSGaqP"
      },
      "outputs": [],
      "source": [
        "main_df.loc[:, 'Review'] = main_df.loc[:, 'Review'].str.lower()\n",
        "main_df['Review']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyq_GtUEFdL2"
      },
      "source": [
        "### **Removal Special Characters**\n",
        "Menghapus karakter khusus atau simbol yang tidak relevan atau tidak diinginkan dari teks.\n",
        "* Menghapus Angka\n",
        "* Menghapus Tanda Baca\n",
        "* Menghapus Garis Baru\n",
        "* Menghapus Spasi Tambahan Di Awal dan Akhir Teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMGRf1z9dSQH"
      },
      "outputs": [],
      "source": [
        "#Menghapus Angka\n",
        "for i in range(len(main_df)):\n",
        "    main_df.loc[i, 'Review'] = re.sub(r'\\d+', '', str(main_df.loc[i, 'Review']))\n",
        "main_df['Review']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m7AKLoJfNjk"
      },
      "outputs": [],
      "source": [
        "# Menghapus Tanda Baca\n",
        "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "for i in range(len(main_df)):\n",
        "    text = main_df.loc[i, 'Review']\n",
        "    for char in punc:\n",
        "        text = text.replace(char, '')\n",
        "    main_df.loc[i, 'Review'] = text\n",
        "main_df['Review']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k29K6FcJg_Nb"
      },
      "outputs": [],
      "source": [
        "# Menghapus Mention dan Hastag\n",
        "for i in range(len(main_df)):\n",
        "    main_df.loc[i, 'Review'] = re.sub(r'@[A-Za-z0-9]+', '', main_df.loc[i, 'Review'])\n",
        "    main_df.loc[i, 'Review'] = re.sub(r'#[A-Za-z0-9]+', '', main_df.loc[i, 'Review'])\n",
        "main_df['Review']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "werTK02gU3qJ"
      },
      "outputs": [],
      "source": [
        "# Menghapus dan Spasi\n",
        "main_df['Review'] = main_df.loc[:, 'Review'].str.strip()\n",
        "main_df['Review']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgPfbTmgF0mV"
      },
      "source": [
        "### **Stopword Removal (Filtering)**\n",
        "Menghapus kata-kata yang umumnya tidak memberikan nilai tambah dalam analisis teks, seperti \"dan\", \"atau\", \"yang\", dll."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Indonesia"
      ],
      "metadata": {
        "id": "XEQTtctJvTvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### English"
      ],
      "metadata": {
        "id": "-nh6g7zqvXEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "Bkk8-wokyYI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filteringText(text):\n",
        "    listStopwords = set(stopwords.words('english'))\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:\n",
        "            filtered.append(txt)\n",
        "    text = filtered\n",
        "    return text\n",
        "\n",
        "def stemmingText(text):\n",
        "    words = text.split()\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    return stemmed_text\n",
        "\n",
        "def toSentence(list_words):\n",
        "    sentence = ' '.join(word for word in list_words)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "6t1-ixEhvarP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QtORwDGCqN"
      },
      "source": [
        "### **Stemming**\n",
        "Proses menghapus imbuhan dari kata untuk mengembalikannya ke bentuk dasarnya. Misalnya, mengubah \"berlari\", \"berlarian\" menjadi \"lari\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIjTgWO-F9w0"
      },
      "source": [
        "### **Tokenizing**\n",
        "Proses membagi teks menjadi bagian-bagian lebih kecil yang disebut token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuXCGpIIGGlc"
      },
      "source": [
        "### **Lemmatization**\n",
        "Proses mengubah kata-kata ke bentuk dasarnya (lema) dengan mempertimbangkan konteks dan struktur bahasa. Misalnya, mengubah \"menyanyikan\" menjadi \"nyanyi\"."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labeling"
      ],
      "metadata": {
        "id": "G1yiGeuZo5rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### English"
      ],
      "metadata": {
        "id": "Sl4MTbFbo1KD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://thecleverprogrammer.com/2021/11/24/add-labels-to-a-dataset-for-sentiment-analysis/"
      ],
      "metadata": {
        "id": "jfJ6Mb8HORPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download(\"vader_lexicon\")"
      ],
      "metadata": {
        "id": "GdSH7Fm6Nxi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiments = SentimentIntensityAnalyzer()\n",
        "main_df[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in main_df[\"Review\"]]\n",
        "main_df[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in main_df[\"Review\"]]\n",
        "main_df[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in main_df[\"Review\"]]\n",
        "main_df['Compound'] = [sentiments.polarity_scores(i)[\"compound\"] for i in main_df[\"Review\"]]\n",
        "main_df.head()"
      ],
      "metadata": {
        "id": "Zp54al8QNyb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = main_df[\"Compound\"].values\n",
        "sentiment = []\n",
        "for i in score:\n",
        "    if i >= 0.05 :\n",
        "        sentiment.append('Positive')\n",
        "    elif i <= -0.05 :\n",
        "        sentiment.append('Negative')\n",
        "    else:\n",
        "        sentiment.append('Neutral')\n",
        "main_df[\"Sentiment\"] = sentiment\n",
        "main_df.head()"
      ],
      "metadata": {
        "id": "gdmD-LEtN45J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(main_df['Sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "tmL8kLFiOymn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indonesia"
      ],
      "metadata": {
        "id": "TU8Yck-hOu4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# Membaca data kamus kata-kata positif dari GitHub\n",
        "lexicon_positive = dict()\n",
        "\n",
        "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
        "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Jika permintaan berhasil\n",
        "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
        "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
        "\n",
        "    for row in reader:\n",
        "        # Mengulangi setiap baris dalam file CSV\n",
        "        lexicon_positive[row[0]] = int(row[1])\n",
        "        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive\n",
        "else:\n",
        "    print(\"Failed to fetch positive lexicon data\")\n",
        "\n",
        "# Membaca data kamus kata-kata negatif dari GitHub\n",
        "lexicon_negative = dict()\n",
        "\n",
        "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
        "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Jika permintaan berhasil\n",
        "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
        "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
        "\n",
        "    for row in reader:\n",
        "        # Mengulangi setiap baris dalam file CSV\n",
        "        lexicon_negative[row[0]] = int(row[1])\n",
        "        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative\n",
        "else:\n",
        "    print(\"Failed to fetch negative lexicon data\")"
      ],
      "metadata": {
        "id": "XABUZq41o9g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menentukan polaritas sentimen dari tweet\n",
        "\n",
        "def sentiment_analysis_lexicon_indonesia(text):\n",
        "    #for word in text:\n",
        "\n",
        "    score = 0\n",
        "    # Inisialisasi skor sentimen ke 0\n",
        "\n",
        "    for word in text:\n",
        "        # Mengulangi setiap kata dalam teks\n",
        "\n",
        "        if (word in lexicon_positive):\n",
        "            score = score + lexicon_positive[word]\n",
        "            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen\n",
        "\n",
        "    for word in text:\n",
        "        # Mengulangi setiap kata dalam teks (sekali lagi)\n",
        "\n",
        "        if (word in lexicon_negative):\n",
        "            score = score + lexicon_negative[word]\n",
        "            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen\n",
        "\n",
        "    polarity=''\n",
        "    # Inisialisasi variabel polaritas\n",
        "\n",
        "    if (score >= 0):\n",
        "        polarity = 'positive'\n",
        "        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif\n",
        "    elif (score < 0):\n",
        "        polarity = 'negative'\n",
        "        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif\n",
        "    else:\n",
        "        polarity = 'neutral'\n",
        "    # Ini adalah bagian yang bisa digunakan untuk menentukan polaritas netral jika diperlukan\n",
        "\n",
        "    return score, polarity\n",
        "    # Mengembalikan skor sentimen dan polaritas teks"
      ],
      "metadata": {
        "id": "qKKXgY-uo-Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = main_df['content'].apply(sentiment_analysis_lexicon_indonesia)\n",
        "results = list(zip(*results))\n",
        "main_df['polarity_score'] = results[0]\n",
        "main_df['polarity'] = results[1]\n",
        "print(main_df['polarity'].value_counts())"
      ],
      "metadata": {
        "id": "PJ7wsjvEpC3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ekstrasi Fitur"
      ],
      "metadata": {
        "id": "hfj5GknkosAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "KHJ5cedtPWia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "V3fqXNP2KWPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = main_df['Review']\n",
        "y = main_df['Sentiment']"
      ],
      "metadata": {
        "id": "IHYoIHLsPEKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )\n",
        "X_tfidf = tfidf.fit_transform(X)"
      ],
      "metadata": {
        "id": "qgGg8BAYPQTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
        "features_df"
      ],
      "metadata": {
        "id": "3ARSLrTTPSvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "EDtm3PXaPtif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bfUQ-3iIPdBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling (Machine Learning)"
      ],
      "metadata": {
        "id": "m1pLkmvRpJPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "EZvPdYi6PjRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models={'LogisticRegression()':LogisticRegression(),\n",
        "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "        'Decision Tree':DecisionTreeClassifier(),\n",
        "        'Support Vector Machine(Linear Kernel)':LinearSVC(),\n",
        "        'Support Vector Machine(Non-Linear Kernal)':SVC(),\n",
        "        'Neural Network':MLPClassifier(),\n",
        "        'Random Forest':RandomForestClassifier(),\n",
        "        'Gradient Boosting':GradientBoostingClassifier()}"
      ],
      "metadata": {
        "id": "vlCRv5uDPydy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in models.items():\n",
        "    print(name)\n",
        "    model.fit(X_train,y_train)\n",
        "    print(model.score(X_test,y_test))"
      ],
      "metadata": {
        "id": "dxYkJYtiP7Re"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}